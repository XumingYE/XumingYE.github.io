---
title: 'PyramidKV阅读'
date: 2025-03-16
permalink: /posts/2025/03/blog-post-1/
tags:
  - KV Cache
  - 大模型推理
  - LLM
---

# 未被缓存KV的token是否参与注意力计算？
这一部分在PyramidKV论文中有比较好的公式化说明，**未被缓存的KV不参与推理**：

![摘自PyramidKV原文](https://cdn.nlark.com/yuque/0/2025/png/26280077/1742090696444-b02337e1-7a80-4791-ab63-b1379bf6ecc9.png)

### **StreamingLLM 如何处理未缓存的 Token？**
在 **StreamingLLM** 中，未缓存的 Token 的 Key（K）和 Value（V）确实会被 **直接忽略**，但这并非简单的“丢弃”，而是通过 **缓存筛选机制** 动态控制参与注意力计算的 Token 范围。以下是具体解释：

---

### **1. StreamingLLM 的核心机制**
#### **缓存策略**
+ **保留两类 Token**：  
    1. **初始 Token（Attention Sink）**：固定保留前 4-8 个 Token 的 KV（如句首的 `<BOS>` 或换行符），作为注意力汇聚点。  
    2. **近期 Token**：保留最新生成的 N 个 Token 的 KV（例如最近的 256 个 Token），维持局部上下文连贯性。
+ **动态淘汰旧 Token**：当输入长度超过缓存容量时，淘汰中间的非关键 Token（仅保留初始和近期 Token）。

#### **注意力计算**
+ **仅使用缓存的 KV**：在计算注意力时，Query（Q）仅与 **缓存中的 K** 计算相似度，未缓存的 Token 的 K 不参与计算。  
+ **Softmax 归一化范围**：注意力分数仅基于缓存中的 Token，Softmax 分母为缓存内所有 Token 的分数之和。  
+ **输出计算**：注意力权重仅作用于缓存的 V，未缓存的 Token 对输出无贡献。

**示例**：假设缓存保留前 4 个 Token 和最近 256 个 Token，输入总长为 1000 Token，则第 5-744 个 Token 的 KV 被丢弃，注意力计算时仅使用前 4 和最后 256 个 Token 的 KV。

---

### **2. 为何能忽略中间 Token？**
#### **注意力汇聚现象**
+ **Softmax 的归一化约束**：即使中间 Token 被移除，模型仍会将多余的注意力分配到初始 Token（Attention Sink），维持注意力分布的稳定性。  
+ **位置偏差**：初始 Token 在训练中被所有后续 Token 可见，模型默认依赖这些位置作为“冗余注意力”的容器。

#### **实验验证**
+ **长文本生成效果**：即使中间 90% 的 Token 被移除，仅保留初始和近期 Token 的缓存，模型仍能生成连贯的文本（如多轮对话）。  
+ **性能恢复**：研究发现，保留初始 Token 的 KV 可恢复 95% 以上的原始模型性能。

---

### **3. 与其他方法的对比**
| **策略** | **缓存范围** | **注意力计算范围** | **优点** | **缺点** |
| --- | --- | --- | --- | --- |
| **Full Attention** | 所有历史 Token | 全部 Token | 精度无损失 | 显存爆炸（O(N²) 复杂度） |
| **Window Attention** | 仅保留近期 N 个 Token | 近期 Token | 显存可控 | 丢失长期依赖，性能骤降 |
| **StreamingLLM** | 初始 Token + 近期 Token | 初始 Token + 近期 Token | 显存可控，性能稳定 | 依赖初始 Token 作为冗余注意力容器 |
| **PyramidKV** | 分层动态调整（底层多，高层少） | 每层保留不同数量的关键 Token | 兼顾长短期依赖，显存优化更灵活 | 实现复杂，需预训练模型支持 |


### **4. 与其他 KV 压缩策略的对比**
| **策略** | **核心思想** | **适用场景** | **局限性** |
| --- | --- | --- | --- |
| **PyramidKV** | 分层压缩，底层保留多、高层保留少 | 长文本生成、多轮对话 | 依赖分层注意力模式，需预训练模型支持 |
| **StreamingLLM** | 固定保留初始 token + 近期 token | 流式生成、低显存设备 | 上下文强依赖时性能下降 |
| **SnapKV** | 基于观察窗口动态选择关键 token | 复杂 Prompt 压缩 | 仅处理 Prompt，生成阶段未优化 |
| **H2O** | 利用 Q-K 矩阵稀疏性，筛选非零值对应 token | 高稀疏性场景 | 稀疏性假设可能不成立 |


---

# 核心算法
### （1）KV Cache的个数
关于PyramidKV的核心算法，真是少之又少，但也可以说是简洁有效。

![每层KV Cache分配公式](https://cdn.nlark.com/yuque/0/2025/png/26280077/1742091483636-1655ace6-0b34-4290-9788-09d90b4b79b9.png)

其中$ k^l $表示第$ l $层所分配的KV缓存个数，$ m $是总层数，$ k^0=(2*k^{total})/m $，$ k^{m-1}=k^{total}/(\beta*m) $

**没有什么好说的，最底层以及最上层的KV Cache数量定好之后，通过公式化代表中间层的KV Cache个数，常用做法**



### （2）KV Cache的选择
这一部分直接套用了SnapKV的选择算法，但是我之前没读过SnapKV，在这仔细介绍一下SnapKV（其实同样很简单）

在Inference的时候，已有的KV算法都会保存 _the last few tokens _作为instruction tokens/query/local window，说明这些token的重要性很高

因此可以通过这些tokens的注意力分布反推其他token的重要性

![画板](https://cdn.nlark.com/yuque/0/2025/jpeg/26280077/1742093851282-af857d80-1522-416f-b7b8-12c8982c7fe4.jpeg)

$ s_i^h = \sum_{j \in [n - \alpha, n]} A_{ij}^h $

j就表明了local window的下标，A表示注意力矩阵

# 代码细节
<font style="color:rgb(31, 35, 40);">SnapKV和PyramidKV确实是只在prefill时被调用。原始的H2O和StreamingLLM也确实是动态维持一个KV cache序列，所以在decoding time和prefill time都会调用。为了公平对比，文中汇报的H2O和StreamingLLM，从它们的原始实现做了改动，也是只在prefill阶段调用。改动以后的H2O和StreamingLLM也会超过设置的上限，这样比起直接使用原始baseline相对恰当。</font>

<font style="color:rgb(31, 35, 40);">目前来看，SnapKV和PyramidKV都是在普通的attention是这种实现</font>

![代码细节](https://cdn.nlark.com/yuque/0/2025/png/26280077/1742132763324-a0799f8e-98cb-47b7-97c5-0b28f90e8d46.png)

<font style="color:rgb(31, 35, 40);">在自回归生成任务时，根据 </font>`<font style="color:rgb(31, 35, 40);">llama_model.py: 92-98</font>`<font style="color:rgb(31, 35, 40);"> 的实现，似乎只在最开始进行 prefill，即</font>`<font style="color:rgb(31, 35, 40);">self.kv_seq_len == 0 </font>`<font style="color:rgb(31, 35, 40);">时能够进入 </font>`<font style="color:rgb(31, 35, 40);">if key_states.shape[-2] == kv_seq_len:</font>`<font style="color:rgb(31, 35, 40);"> 分支，从而调用 kv_cluster.update_kv(…)</font>  
<font style="color:rgb(31, 35, 40);">在之后的 decoding 过程中每次forward新输入的seq_len（即 </font>`<font style="color:rgb(31, 35, 40);">key_states.shape[-2]</font>`<font style="color:rgb(31, 35, 40);">）为1，总不等于累加上 KVCache长度的 </font>`<font style="color:rgb(31, 35, 40);">kv_seq_len</font>`<font style="color:rgb(31, 35, 40);">。故 kv_cluster.update_kv 不再被调用，每次新生成的 k/v 都会保留下来。</font>

> 上述回复正确
>

[https://github.com/Zefan-Cai/KVCache-Factory/issues/20](https://github.com/Zefan-Cai/KVCache-Factory/issues/20)